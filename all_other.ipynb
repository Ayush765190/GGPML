{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmean clustering\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "data = make_blobs(n_samples=200,n_features=2,centers=4,cluster_std=1.8,random_state=101)\n",
    "\n",
    "data[0]\n",
    "\n",
    "data[1]\n",
    "\n",
    "plt.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "kmeans.fit(data[0])\n",
    "kmeans.cluster_centers_\n",
    "\n",
    "kmeans.labels_\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(data[1],kmeans.labels_))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,sharey=True,figsize=(10,6))\n",
    "ax1.set_title('K Means')\n",
    "ax1.scatter(data[0][:,0],data[0][:,1],c=kmeans.labels_,cmap='rainbow')\n",
    "\n",
    "ax2.set_title('Original')\n",
    "ax2.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')\n",
    "\n",
    "\n",
    "#Knn sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/Iris/iri_data.csv\",header=None)\n",
    "df.head()\n",
    "\n",
    "df.columns = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"]\n",
    "\n",
    "df.head()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(df.drop('class',axis=1))\n",
    "\n",
    "scaled_features = scaler.transform(df.drop('class',axis=1))\n",
    "scaled_features\n",
    "\n",
    "df_feat = pd.DataFrame(scaled_features,columns = df.columns[:-1])\n",
    "df_feat.head()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x=df_feat\n",
    "y=df['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=101)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn.fit(x_train,y_train)\n",
    "\n",
    "pred = knn.predict(x_test)\n",
    "pred\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"Accuracy->\",accuracy_score(y_test,pred))\n",
    "\n",
    "#AND and OR\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the AND gate truth table\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Initialize the weights and biases\n",
    "np.random.seed(0)\n",
    "weights_and = np.random.randn(2, 1)\n",
    "bias_and = np.random.randn(1)\n",
    "\n",
    "# Define the activation function (sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    output_and = sigmoid(np.dot(X_and, weights_and) + bias_and)\n",
    "    \n",
    "    # Compute the loss (mean squared error)\n",
    "    loss_and = np.mean((output_and - y_and) ** 2)\n",
    "    \n",
    "    # Backpropagation\n",
    "    d_loss_and = 2 * (output_and - y_and) / len(y_and)\n",
    "    d_output_and = sigmoid_derivative(output_and)\n",
    "    d_weights_and = np.dot(X_and.T, d_loss_and * d_output_and)\n",
    "    d_bias_and = np.sum(d_loss_and * d_output_and)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    weights_and -= learning_rate * d_weights_and\n",
    "    bias_and -= learning_rate * d_bias_and\n",
    "\n",
    "# Test the model with some examples\n",
    "test_data_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "output_and_test = sigmoid(np.dot(test_data_and, weights_and) + bias_and)\n",
    "print(\"AND Gate Predictions:\")\n",
    "print(output_and_test)\n",
    "\n",
    "\n",
    "#XOR\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "X = np.array([\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "y = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "num_i_units = 2\n",
    "num_h_units = 2\n",
    "num_o_units = 1\n",
    "\n",
    "learning_rate = 0.1 # 0.001, 0.01 <- Magic values\n",
    "reg_param = 0 # 0.001, 0.01 <- Magic values\n",
    "max_iter = 5000 # 5000 <- Magic value\n",
    "m = 4 # Number of training examples\n",
    "\n",
    "# The model needs to be over fit to make predictions. Which \n",
    "np.random.seed(1)\n",
    "W1 = np.random.normal(0, 1, (num_h_units, num_i_units)) # 2x2\n",
    "W2 = np.random.normal(0, 1, (num_o_units, num_h_units)) # 1x2\n",
    "\n",
    "B1 = np.random.random((num_h_units, 1)) # 2x1\n",
    "B2 = np.random.random((num_o_units, 1)) # 1x1\n",
    "\n",
    "def sigmoid(z, derv=False):\n",
    "    if derv: return z * (1 - z)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward(x, predict=False):\n",
    "    a1 = x.reshape(x.shape[0], 1) # Getting the training example as a column vector.\n",
    "\n",
    "    z2 = W1.dot(a1) + B1 # 2x2 * 2x1 + 2x1 = 2x1\n",
    "    a2 = sigmoid(z2) # 2x1\n",
    "\n",
    "    z3 = W2.dot(a2) + B2 # 1x2 * 2x1 + 1x1 = 1x1\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    if predict: return a3\n",
    "    return (a1, a2, a3)\n",
    "\n",
    "dW1 = 0\n",
    "dW2 = 0\n",
    "\n",
    "dB1 = 0\n",
    "dB2 = 0\n",
    "\n",
    "cost = np.zeros((max_iter, 1))\n",
    "for i in range(max_iter):\n",
    "    c = 0\n",
    "\n",
    "    dW1 = 0\n",
    "    dW2 = 0\n",
    "\n",
    "    dB1 = 0\n",
    "    dB2 = 0\n",
    "    for j in range(m):\n",
    "        sys.stdout.write(\"\\rIteration: {} and {}\".format(i + 1, j + 1))\n",
    "\n",
    "        # Forward Prop.\n",
    "        a0 = X[j].reshape(X[j].shape[0], 1) # 2x1\n",
    "\n",
    "        z1 = W1.dot(a0) + B1 # 2x2 * 2x1 + 2x1 = 2x1\n",
    "        a1 = sigmoid(z1) # 2x1\n",
    "\n",
    "        z2 = W2.dot(a1) + B2 # 1x2 * 2x1 + 1x1 = 1x1\n",
    "        a2 = sigmoid(z2) # 1x1\n",
    "\n",
    "        # Back prop.\n",
    "        dz2 = a2 - y[j] # 1x1\n",
    "        dW2 += dz2 * a1.T # 1x1 .* 1x2 = 1x2\n",
    "\n",
    "        dz1 = np.multiply((W2.T * dz2), sigmoid(a1, derv=True)) # (2x1 * 1x1) .* 2x1 = 2x1\n",
    "        dW1 += dz1.dot(a0.T) # 2x1 * 1x2 = 2x2\n",
    "\n",
    "        dB1 += dz1 # 2x1\n",
    "        dB2 += dz2 # 1x1\n",
    "\n",
    "        c = c + (-(y[j] * np.log(a2)) - ((1 - y[j]) * np.log(1 - a2)))\n",
    "        sys.stdout.flush() # Updating the text.\n",
    "    W1 = W1 - learning_rate * (dW1 / m) + ( (reg_param / m) * W1)\n",
    "    W2 = W2 - learning_rate * (dW2 / m) + ( (reg_param / m) * W2)\n",
    "\n",
    "    B1 = B1 - learning_rate * (dB1 / m)\n",
    "    B2 = B2 - learning_rate * (dB2 / m)\n",
    "    cost[i] = (c / m) + ( \n",
    "        (reg_param / (2 * m)) * \n",
    "        (\n",
    "            np.sum(np.power(W1, 2)) + \n",
    "            np.sum(np.power(W2, 2))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "for x in X:\n",
    "    print(\"\\n\")\n",
    "    print(x)\n",
    "    print(forward(x, predict=True))\n",
    "\n",
    "plt.plot(range(max_iter), cost)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#decisison tree sklearn\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "#A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x_train,y_train)\n",
    "\n",
    "tree.plot_tree(clf)\n",
    "\n",
    "predictions = clf.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "\n",
    "## SVM (Support Vector Machine)\n",
    "\n",
    "# Support Vector Machine(SVM) is a supervised machine learning algorithm used for both classification and regression. Though we say regression problems as well its best suited for classification. The objective of SVM algorithm is to find a hyperplane in an N-dimensional space that distinctly classifies the data points. The dimension of the hyperplane depends upon the number of features. If the number of input features is two, then the hyperplane is just a line. If the number of input features is three, then the hyperplane becomes a 2-D plane. It becomes difficult to imagine when the number of features exceeds three. \n",
    "\n",
    "# The main motive behind selecting a hyper plane is to maximise the margin between the plain and the classes(or you can say the datapoints of the classes)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "cancer.keys()\n",
    "\n",
    "df_feat = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
    "\n",
    "df_feat.head()\n",
    "\n",
    "cancer['target']\n",
    "\n",
    "print(cancer['target_names'])\n",
    "x = df_feat\n",
    "y = cancer['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "\n",
    "##SVM class\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "#import pandas\n",
    "import pandas as pd\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"diabetes.csv\", header=1, names=col_names)\n",
    "\n",
    "#split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable\n",
    "\n",
    "\n",
    "pima.describe()\n",
    "\n",
    "\n",
    "print(X,y)\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "print(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "#model=svm.SVC(kernel='rbf',gamma=0.5,C=5)\n",
    "\n",
    "from sklearn import svm\n",
    "model=svm.SVC(kernel='rbf',gamma=10,C=10)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "accuracy=((77)/(77+38))*100\n",
    "print(\"Accuracy=\",accuracy)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = {\n",
    "\n",
    "    'C':[0.01,0.1,1],\n",
    "\n",
    "    'kernel' : [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n",
    "\n",
    "    'degree' : [1,3,5,7],\n",
    "\n",
    "    'gamma' : [0.01,2,5]\n",
    "\n",
    "}\n",
    "\n",
    "svm  = SVC()\n",
    "\n",
    "svm_cv = GridSearchCV(svm, grid, cv = 5)\n",
    "\n",
    "svm_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"Best Parameters:\",svm_cv.best_params_)\n",
    "\n",
    "print(\"Train Score:\",svm_cv.best_score_)\n",
    "\n",
    "print(\"Test Score:\",svm_cv.score(X_test,y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
