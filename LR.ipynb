{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#liner regression using normal eq one var\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([4,6,8,2,5])\n",
    "y = np.array([17,23,34,9,13])\n",
    "print(\"Array X: \",x)\n",
    "print(\"Array Y: \",y)\n",
    "\n",
    "n = np.size(x)\n",
    "n\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel(\"X datapoints\")\n",
    "plt.ylabel(\"Y datapoints\")\n",
    "plt.title(\"Plotting Input data\")\n",
    "plt.show()\n",
    "\n",
    "x_bias = np.ones((n,1))\n",
    "x_bias\n",
    "\n",
    "print(x.shape)\n",
    "print(x_bias.shape)\n",
    "\n",
    "x_new = x.reshape([-1,1])\n",
    "x_new\n",
    "\n",
    "#forming a complete matrix of bias and values\n",
    "x_new = np.append(x_bias, x_new, axis = 1)\n",
    "x_new\n",
    "\n",
    "x_new_transpose = x_new.transpose()\n",
    "x_new_transpose\n",
    "\n",
    "x_new_transpose_dot_x_new = x_new_transpose.dot(x_new)\n",
    "x_new_transpose_dot_x_new\n",
    "\n",
    "#finding inverse of the matrix\n",
    "temp_1 = np.linalg.inv(x_new_transpose_dot_x_new)\n",
    "temp_1\n",
    "\n",
    "#finding the dot product of transposed X and Y\n",
    "temp_2 = x_new_transpose.dot(y)\n",
    "temp_2\n",
    "\n",
    "\n",
    "#finding coefficients\n",
    "theta = temp_1.dot(temp_2)\n",
    "theta\n",
    "\n",
    "Intercept = theta[0]\n",
    "Slope =  theta[1]\n",
    "print(\"Intercept: \",Intercept)\n",
    "print(\"Slope: \",Slope)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel(\"X datapoints\")\n",
    "plt.ylabel(\"Y datapoints\")\n",
    "plt.title(\"Plotting Input data and Hypothesis\")\n",
    "plt.plot(x,Slope*x+Intercept,color='red')\n",
    "\n",
    "\n",
    "#Value prediction \n",
    "y_pred = Intercept+Slope*x\n",
    "print(\"Predicted Values of Y: \",y_pred)\n",
    "\n",
    "error = y - y_pred\n",
    "se = np.sum(error**2)\n",
    "mse = se/n\n",
    "print(\"Mean Squared error: \",mse)\n",
    "print(\"Root mean Squared error: \",np.sqrt(mse))\n",
    "\n",
    "#Qlinear regression using normal equation for multivar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('datasets/USA_Housing.csv')\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.columns\n",
    "\n",
    "x=data[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n",
    "       'Avg. Area Number of Bedrooms', 'Area Population']]\n",
    "x.head()\n",
    "\n",
    "y=data['Price']\n",
    "y.head()\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "n = data.shape[0]\n",
    "print(\"No of data: \",n)\n",
    "\n",
    "x_bias = np.ones((n,1))\n",
    "x_new = x_bias\n",
    "\n",
    "for (columnName, columnData) in x.iteritems():\n",
    "    tempArr = np.array(columnData)\n",
    "    tempNew = tempArr.reshape([-1,1])\n",
    "    x_new = np.append(x_new,tempNew,axis=1)\n",
    "    \n",
    "\n",
    "x_new\n",
    "\n",
    "x_new_transpose = np.transpose(x_new)\n",
    "\n",
    "x_new_transpose_dot_x_new = x_new_transpose.dot(x_new)\n",
    "#finding inverse of the matrix\n",
    "\n",
    "temp_1 = np.linalg.inv(x_new_transpose_dot_x_new)\n",
    "temp_1\n",
    "\n",
    "\n",
    "#finding the dot product of transposed X and Y\n",
    "\n",
    "temp_2 = x_new_transpose.dot(y)\n",
    "temp_2\n",
    "\n",
    "#finding coefficients\n",
    "\n",
    "theta = temp_1.dot(temp_2)\n",
    "theta\n",
    "\n",
    "def predict_values(theta, test_data):\n",
    "    y = []\n",
    "    for ind in test_data.index:\n",
    "        y.append(theta[0]) \n",
    "        for idx, column in enumerate(test_data.columns):\n",
    "            y[ind] = y[ind]+theta[idx+1]*test_data[column][ind]\n",
    "    return y\n",
    "\n",
    "xyz = data[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n",
    "       'Avg. Area Number of Bedrooms', 'Area Population']]\n",
    "predicted_y = predict_values(theta, xyz)\n",
    "\n",
    "error = y - predicted_y\n",
    "absolute_error = np.sum(error)\n",
    "mean_absolute_error = absolute_error/n\n",
    "se = np.sum(error**2)\n",
    "mse = se/n\n",
    "print(\"Mean absolute error: \",mean_absolute_error)\n",
    "print(\"Mean Squared error: \",mse)\n",
    "print(\"Root mean Squared error: \",np.sqrt(mse))\n",
    "\n",
    "#Q3linear regression using sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv('datasets/USA_Housing.csv')\n",
    "df.head()\n",
    "\n",
    "x=df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n",
    "       'Avg. Area Number of Bedrooms', 'Area Population']]\n",
    "x.head()\n",
    "\n",
    "y=df['Price']\n",
    "y.head()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.4,random_state=101)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm\n",
    "\n",
    "lm.fit(x_train,y_train)\n",
    "\n",
    "lm.intercept_\n",
    "\n",
    "lm.coef_\n",
    "\n",
    "cdf = pd.DataFrame(lm.coef_,x.columns,columns=['coeff'])\n",
    "cdf\n",
    "\n",
    "predictions = lm.predict(x_test)\n",
    "predictions\n",
    "\n",
    "y_test\n",
    "\n",
    "plt.scatter(y_test,predictions)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "metrics.mean_absolute_error(y_test,predictions)\n",
    "\n",
    "\n",
    "metrics.mean_squared_error(y_test,predictions)\n",
    "\n",
    "#LR using GD\n",
    "\n",
    "# Making the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (12.0, 9.0)\n",
    "\n",
    "\n",
    "X = np.array([1,2,3,4,5])\n",
    "Y = np.array([7,14,15,18,19])\n",
    "plt.scatter(X, Y)\n",
    "plt.show()\n",
    "\n",
    "# Building the model\n",
    "m = 0\n",
    "c = 0\n",
    "\n",
    "L = 0.0001  # The learning Rate\n",
    "epochs = 1000  # The number of iterations to perform gradient descent\n",
    "\n",
    "n = float(len(X)) # Number of elements in X\n",
    "\n",
    "# Performing Gradient Descent \n",
    "for i in range(epochs): \n",
    "    Y_pred = m*X + c  # The current predicted value of Y\n",
    "    D_m = (-2/n) * sum(X * (Y - Y_pred))  # Derivative wrt m\n",
    "    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c\n",
    "    m = m - L * D_m  # Update m\n",
    "    c = c - L * D_c  # Update c\n",
    "    \n",
    "print (m, c)\n",
    "\n",
    "\n",
    "# Making predictions\n",
    "Y_pred = m*X + c\n",
    "plt.scatter(X, Y)\n",
    "plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')  # regression line\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#LR manual\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# Data Generate X,Y \n",
    "def generateDataset(m):\n",
    "    X = np.random.randn(m)*10\n",
    "    noise = np.random.randn(m)\n",
    "    y = 3*X + 1 + 5*noise\n",
    "    return X,y\n",
    "\n",
    "X,y = generateDataset(100)\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "def plotData(X,y,color=\"orange\",title=\"Data\"):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.scatter(X,y,c=color)\n",
    "    plt.show()\n",
    "\n",
    "plotData(X,y)\n",
    "\n",
    "\n",
    "def normaliseData(X):\n",
    "    X = (X-X.mean())/X.std()\n",
    "    return X\n",
    "\n",
    "plotData(X,y)\n",
    "X = normaliseData(X)\n",
    "plotData(X,y)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "XT, Xt, yT, yt = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(XT.shape,yT.shape)\n",
    "print(Xt.shape,yt.shape)\n",
    "\n",
    "\n",
    "plt.scatter(XT,yT,color='orange',label=\"Train Data\")\n",
    "plt.scatter(Xt,yt,color='blue',label=\"Test Data\")\n",
    "plt.title(\"Train-Test Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def hypothesis(X,theta):\n",
    "    return theta[0] + theta[1]*X \n",
    "\n",
    "def error(X,y,theta):\n",
    "    m = X.shape[0]\n",
    "    e = 0 \n",
    "    for i in range(m):\n",
    "        y_i = hypothesis(X[i],theta)\n",
    "        e = e + (y[i] - y_i)**2 \n",
    "    \n",
    "    return e/(2*m)\n",
    "\n",
    "def gradient(X,y,theta):\n",
    "    m = X.shape[0]\n",
    "    grad = np.zeros((2,))\n",
    "\n",
    "    for i in range(m):\n",
    "        exp = hypothesis(X[i],theta) - y[i]\n",
    "        grad[0] += (exp) \n",
    "        grad[1] += (exp)*X[i]\n",
    "\n",
    "    return grad/m \n",
    "\n",
    "\n",
    "def train(X,y,learning_rate = 0.1,maxItrs = 100):\n",
    "    theta = np.zeros((2,))\n",
    "    error_list = []\n",
    "    \n",
    "    for i in range(maxItrs):\n",
    "        grad = gradient(X,y,theta)\n",
    "        error_list.append(error(X,y,theta))\n",
    "        theta[0] = theta[0] - learning_rate * grad[0]\n",
    "        theta[1] = theta[1] - learning_rate * grad[1]\n",
    "\n",
    "    plt.xlabel(\"Iteration Number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(error_list)\n",
    "    return theta\n",
    "\n",
    "\n",
    "theta = train(X,y)\n",
    "\n",
    "theta\n",
    "\n",
    "def predict(X,theta):\n",
    "    return hypothesis(X,theta)\n",
    "\n",
    "plt.scatter(XT,yT)\n",
    "plt.scatter(Xt,yt,color='orange')\n",
    "plt.show()\n",
    "\n",
    "yp = predict(Xt,theta)\n",
    "\n",
    "plt.scatter(XT,yT,label=\"train\")\n",
    "plt.scatter(Xt,yt,color='orange',label=\"test\")\n",
    "plt.plot(Xt,yp,color='green',label=\"prediction\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Model Evaluation\n",
    "def r2Score(y,yp):\n",
    "    ymean = y.mean()\n",
    "    num = np.sum((y-yp)**2)\n",
    "    denom = np.sum((y - ymean)**2)\n",
    "    return 1 - num/denom \n",
    "\n",
    "r2Score(yt,yp)\n",
    "\n",
    "# Visualising the training process \n",
    "T0 = np.arange(-120,150,10)\n",
    "T1 = np.arange(-120,150,10)\n",
    "\n",
    "T0,T1 = np.meshgrid(T0,T1)\n",
    "J = np.zeros(T0.shape)\n",
    "for i in range(J.shape[0]):\n",
    "    for j in range(J.shape[1]):\n",
    "        yp = T1[i,j]*X + T0[i,j]\n",
    "        J[i,j] = np.mean((y-yp)**2)/2\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.gca(projection='3d')\n",
    "axes.plot_surface(T0,T1,J,cmap='rainbow')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.gca(projection='3d')\n",
    "axes.contour(T0,T1,J,cmap='rainbow')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Trace the trajectory of gradient \n",
    "def train(X,y,learning_rate = 0.1,maxItrs = 100):\n",
    "    theta = np.array([-150,100])\n",
    "    error_list = []\n",
    "    # note down values of theta\n",
    "    theta_list = []\n",
    "    \n",
    "    for i in range(maxItrs):\n",
    "        grad = gradient(X,y,theta)\n",
    "        error_list.append(error(X,y,theta))\n",
    "        theta_list.append((theta[0],theta[1]))\n",
    "        theta[0] = theta[0] - learning_rate * grad[0]\n",
    "        theta[1] = theta[1] - learning_rate * grad[1]\n",
    "\n",
    "    return theta, theta_list, error_list\n",
    "\n",
    "theta, theta_list, error_list = train(XT,yT)\n",
    "\n",
    "theta_list = np.array(theta_list)\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.gca(projection='3d')\n",
    "axes.contour(T0,T1,J,cmap='rainbow')\n",
    "axes.scatter(theta_list[:,0], theta_list[:,1],error_list)\n",
    "plt.show()\n",
    "\n",
    "# 2D Contour Plot (Top View)\n",
    "plt.contour(T0,T1,J,cmap='rainbow')\n",
    "plt.scatter(theta_list[:,0],theta_list[:,1],label='Trajectory')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some random data for demonstration\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.show()\n",
    "\n",
    "# Add a bias term to X (X0 = 1)\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "#plt.scatter(X_b,y)\n",
    "#plt.show()\n",
    "print(X_b.shape, y.shape)\n",
    "\n",
    "# Gradient Descent Parameters\n",
    "eta = 0.1  # learning rate\n",
    "n_iterations = 100\n",
    "m = 100  # number of instances\n",
    "\n",
    "# Initialize theta with random values\n",
    "theta = np.random.randn(2, 1)\n",
    "print(theta)\n",
    "print(\"//////\")\n",
    "# Gradient Descent\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    print(theta)\n",
    "# Final theta values (intercept and slope)\n",
    "intercept, slope = theta\n",
    "\n",
    "# Print the final parameters\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"Slope:\", slope)\n",
    "\n",
    "# Plot the data and the linear regression line\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, X_b.dot(theta), color='red')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Linear Regression using Gradient Descent\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##logR using sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/Iris/iri_data.csv\",header=None)\n",
    "df.head()\n",
    "\n",
    "df.columns = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x=df[[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]]\n",
    "y=df['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=101)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(x_train,y_train)\n",
    "\n",
    "predictions = logmodel.predict(x_test)\n",
    "predictions[:10]\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "\n",
    "#logr_manual\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test = pd.read_csv(r\"C:\\Users\\ashuv\\Desktop\\csv\\titanic_test.csv\")\n",
    "train = pd.read_csv(r\"C:\\Users\\ashuv\\Desktop\\csv\\titanic_train.csv\")\n",
    "\n",
    "X_train = train.drop('Name', axis =1)\n",
    "X_train\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "m = X_train.shape[1]\n",
    "n = X_train.shape[0]\n",
    "\n",
    "W = np.zeros((n,1))\n",
    "\n",
    "W.shape\n",
    "\n",
    "def model(X, Y, learning_rate, iterations):\n",
    "    \n",
    "    m = X_train.shape[1]\n",
    "    n = X_train.shape[0]\n",
    "    \n",
    "    W = np.zeros((n,1))\n",
    "    B = 0\n",
    "    \n",
    "    cost_list = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        Z = np.dot(W.T, X) + B\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "        # cost function\n",
    "        cost = -(1/m)*np.sum( Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "        \n",
    "        # Gradient Descent\n",
    "        dW = (1/m)*np.dot(A-Y, X.T)\n",
    "        dB = (1/m)*np.sum(A - Y)\n",
    "        \n",
    "        W = W - learning_rate*dW.T\n",
    "        B = B - learning_rate*dB\n",
    "        \n",
    "        # Keeping track of our cost function value\n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        if(i%(iterations/10) == 0):\n",
    "            print(\"cost after \", i, \"iteration is : \", cost)\n",
    "        \n",
    "    return W, B, cost_list\n",
    "\n",
    "\n",
    "iterations = 100000\n",
    "learning_rate = 0.0015\n",
    "W, B, cost_list = model(X_train, Y_train, learning_rate = learning_rate, iterations = iterations)\n",
    "\n",
    "plt.plot(np.arange(iterations), cost_list)\n",
    "plt.show()\n",
    "\n",
    "def accuracy(X, Y, W, B):\n",
    "    \n",
    "    Z = np.dot(W.T, X) + B\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    A = A > 0.5\n",
    "    \n",
    "    A = np.array(A, dtype = 'int64')\n",
    "    \n",
    "    acc = (1 - np.sum(np.absolute(A - Y))/Y.shape[1])*100\n",
    "    \n",
    "    print(\"Accuracy of the model is : \", round(acc, 2), \"%\")\n",
    " \n",
    "accuracy(X_test, Y_test, W, B)\n",
    "    \n",
    "    \n",
    "#gradient\n",
    "\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_descent(x, y):\n",
    "    m_curr = b_curr = 0;\n",
    "    iterations = 300\n",
    "    \n",
    "    learning_rate = 0.0001;\n",
    "    cost_list=[]\n",
    "    epoch_list=[]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        y_predicted = m_curr * x + b_curr\n",
    "        \n",
    "        m_derivative = -sum(x * (y -  y_predicted))\n",
    "        b_derivative = -sum(y -  y_predicted)\n",
    "        \n",
    "        m_curr = m_curr - learning_rate * m_derivative\n",
    "        b_curr = b_curr - learning_rate * b_derivative\n",
    "        \n",
    "        cost = (1/2) * sum([val ** 2 for val in (y - y_predicted)])\n",
    "        print(\"m {}, iteration {}, cost {}\".format(m_curr, b_curr, i, cost ))\n",
    "        \n",
    "        if(i%10==0):\n",
    "            cost_list.append(cost)\n",
    "            epoch_list.append(i)\n",
    "    return cost_list, epoch_list\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([5, 7, 9, 11, 13])\n",
    "\n",
    "cost_list, epoch_list = gradient_descent(x,y)\n",
    "\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cost\");\n",
    "plt.plot(epoch_list, cost_list)\n",
    "\n",
    "##Naive bayes\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy->\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
